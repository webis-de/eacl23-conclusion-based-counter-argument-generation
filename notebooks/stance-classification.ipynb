{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ee0116-ad25-483e-a9c3-b912fcafd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeb587-32e6-490b-b78b-b6a372a0dee7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load kialo data from scratch \n",
    "#### (scroll down if want to use already processed kialo data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec3f8fc-3d35-44e6-b5d4-e4c5a2ffdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad63585c-f0c6-4585-8865-f66e15745e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c15b74-fc24-4e1d-a832-1b8da75e1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo_ds_path = data_path = '../data/stance-classification-kialo-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ba0b50-33d4-445e-a54d-6b9276cbe1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kialo_df = pd.read_pickle(kialo_ds_path + '/kialo_train_df.pkl')\n",
    "valid_kialo_df = pd.read_pickle(kialo_ds_path + '/kialo_valid_df.pkl')\n",
    "test_kialo_df = pd.read_pickle(kialo_ds_path + '/kialo_test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6907112-2ae8-4182-9a2d-6a0fa1538637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(df):\n",
    "    \n",
    "    df = df.groupby('conclusion_text').agg({\n",
    "        'premises': lambda x: list(x)[0],\n",
    "        'counter' : lambda x: list(x)\n",
    "    }).reset_index()\n",
    "    \n",
    "    output_data = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        for premise in row['premises']:\n",
    "            num_tokens = len(premise.split())\n",
    "            if  num_tokens <= 200 and num_tokens > 3:\n",
    "                output_data.append((row['conclusion_text'], premise, 0))\n",
    "\n",
    "        for counter in row['counter']:\n",
    "            num_tokens = len(counter.split())\n",
    "            if  num_tokens <= 200 and num_tokens > 3:\n",
    "                output_data.append((row['conclusion_text'], counter, 1))\n",
    "\n",
    "    output_df = pd.DataFrame(output_data, columns=['claim1', 'claim2', 'label'])\n",
    "    \n",
    "    #Balancing the dataframe\n",
    "    g = output_df.groupby('label')\n",
    "    output_df = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26723968-6e40-4aab-ae3e-43727ec8b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_df(train_kialo_df)\n",
    "valid_df = create_df(valid_kialo_df)\n",
    "test_df  = create_df(test_kialo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2204b835-1aae-45f7-b31e-c65f402dd087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    47832\n",
       "0    47832\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8874d9cb-4bdd-4325-9f93-4cbd782ed9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3858\n",
       "0    3858\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253c6fc3-91a7-4ae4-a1c8-f1e8ab7e755a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11227\n",
       "0    11227\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab68d92-bb48-423f-87a7-4258b9037988",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(kialo_ds_path + '/kialo_stance_classification_training_data.csv', index=False)\n",
    "test_df.to_csv(kialo_ds_path  + '/kialo_stance_classification_test_data.csv', index=False)\n",
    "valid_df.to_csv(kialo_ds_path +'/kialo_stance_classification_valid_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a848170-5458-42e3-9352-ede0560ba088",
   "metadata": {},
   "source": [
    "## Load already processed kialo data for tokenization and training for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8076231e-e037-46a6-a9c0-446ff5b239b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, RobertaTokenizer, RobertaForSequenceClassification, TextClassificationPipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19aad4a3-541c-4e55-beb8-391612b12bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(data_path + '/kialo_stance_classification_training_data.csv')\n",
    "test_df  = pd.read_csv(data_path + '/kialo_stance_classification_test_data.csv')\n",
    "valid_df = pd.read_csv(data_path + '/kialo_stance_classification_valid_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e453c-7357-4483-a635-8f9b950196b0",
   "metadata": {},
   "source": [
    "### convert df into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fbf9473-df1e-4ba4-a06b-44aae5926a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['input_txt'] = train_df.apply(lambda x: x['claim1'] + ' </s> ' + x['claim2'], axis=1)\n",
    "valid_df['input_txt'] = valid_df.apply(lambda x: x['claim1'] + ' </s> ' + x['claim2'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ae352-20c2-4b32-b7cf-8014e6d18308",
   "metadata": {},
   "source": [
    "counter label is 1\n",
    "support label is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0181c74-2ff5-4365-8de1-c5bb03a135b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim1</th>\n",
       "      <th>claim2</th>\n",
       "      <th>label</th>\n",
       "      <th>input_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87038</th>\n",
       "      <td>There is no grounds to assume evil would not be present in the best of all worlds. Best is relative.</td>\n",
       "      <td>Most believers in an omniscient God also believe that God mandates objective moral standards of good and bad. So claiming that \"best\" is relative is not a viable defense for most believers in God.</td>\n",
       "      <td>1</td>\n",
       "      <td>There is no grounds to assume evil would not be present in the best of all worlds. Best is relative. &lt;/s&gt; Most believers in an omniscient God also believe that God mandates objective moral standards of good and bad. So claiming that \"best\" is relative is not a viable defense for most believers in God.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45013</th>\n",
       "      <td>The creation of a strengthened monopoly of power could harm those who have differing or opposing views to the central government. This is underlined by today's EU behavior.</td>\n",
       "      <td>The President of the European Commission Jean-Claude Juncker told European citizens that Britain will be treated as “deserters following a vote to leave the European Union. This is intimidation.</td>\n",
       "      <td>0</td>\n",
       "      <td>The creation of a strengthened monopoly of power could harm those who have differing or opposing views to the central government. This is underlined by today's EU behavior. &lt;/s&gt; The President of the European Commission Jean-Claude Juncker told European citizens that Britain will be treated as “deserters following a vote to leave the European Union. This is intimidation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38495</th>\n",
       "      <td>Difficulties with the formation of a religious identity may cause children of interfaith marriages to be pushed away from religion in general.</td>\n",
       "      <td>A 2006 survey showed that 37% of those raised by parents of different religions reported weekly attendance at religious services, compared with 42% of those raised by parents with the same faith.</td>\n",
       "      <td>0</td>\n",
       "      <td>Difficulties with the formation of a religious identity may cause children of interfaith marriages to be pushed away from religion in general. &lt;/s&gt; A 2006 survey showed that 37% of those raised by parents of different religions reported weekly attendance at religious services, compared with 42% of those raised by parents with the same faith.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91872</th>\n",
       "      <td>Gambling causes problems for the individual.</td>\n",
       "      <td>The casino owners are also individuals whom are participating in gambling during their daily operations. These people benefit from gambling.</td>\n",
       "      <td>1</td>\n",
       "      <td>Gambling causes problems for the individual. &lt;/s&gt; The casino owners are also individuals whom are participating in gambling during their daily operations. These people benefit from gambling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92051</th>\n",
       "      <td>Religion is good for the psycho-social wellness of its followers.</td>\n",
       "      <td>Religion usually requires prayer, and this is the part that has shown psychological usefulness. In reducing prayer to a psychological technique of objective visualisation, you remove the need for god. This kind of prayer can work without the use of god, for example the placebo effect, or sports visualisation techniques. So these ideas show that we don't need to pray to God in order for the the techniques to work.</td>\n",
       "      <td>1</td>\n",
       "      <td>Religion is good for the psycho-social wellness of its followers. &lt;/s&gt; Religion usually requires prayer, and this is the part that has shown psychological usefulness. In reducing prayer to a psychological technique of objective visualisation, you remove the need for god. This kind of prayer can work without the use of god, for example the placebo effect, or sports visualisation techniques. So these ideas show that we don't need to pray to God in order for the the techniques to work.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                             claim1  \\\n",
       "87038                                                                          There is no grounds to assume evil would not be present in the best of all worlds. Best is relative.   \n",
       "45013  The creation of a strengthened monopoly of power could harm those who have differing or opposing views to the central government. This is underlined by today's EU behavior.   \n",
       "38495                                Difficulties with the formation of a religious identity may cause children of interfaith marriages to be pushed away from religion in general.   \n",
       "91872                                                                                                                                  Gambling causes problems for the individual.   \n",
       "92051                                                                                                             Religion is good for the psycho-social wellness of its followers.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                 claim2  \\\n",
       "87038                                                                                                                                                                                                                              Most believers in an omniscient God also believe that God mandates objective moral standards of good and bad. So claiming that \"best\" is relative is not a viable defense for most believers in God.   \n",
       "45013                                                                                                                                                                                                                                The President of the European Commission Jean-Claude Juncker told European citizens that Britain will be treated as “deserters following a vote to leave the European Union. This is intimidation.   \n",
       "38495                                                                                                                                                                                                                               A 2006 survey showed that 37% of those raised by parents of different religions reported weekly attendance at religious services, compared with 42% of those raised by parents with the same faith.   \n",
       "91872                                                                                                                                                                                                                                                                                      The casino owners are also individuals whom are participating in gambling during their daily operations. These people benefit from gambling.   \n",
       "92051  Religion usually requires prayer, and this is the part that has shown psychological usefulness. In reducing prayer to a psychological technique of objective visualisation, you remove the need for god. This kind of prayer can work without the use of god, for example the placebo effect, or sports visualisation techniques. So these ideas show that we don't need to pray to God in order for the the techniques to work.   \n",
       "\n",
       "       label  \\\n",
       "87038      1   \n",
       "45013      0   \n",
       "38495      0   \n",
       "91872      1   \n",
       "92051      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     input_txt  \n",
       "87038                                                                                                                                                                                           There is no grounds to assume evil would not be present in the best of all worlds. Best is relative. </s> Most believers in an omniscient God also believe that God mandates objective moral standards of good and bad. So claiming that \"best\" is relative is not a viable defense for most believers in God.  \n",
       "45013                                                                                                                     The creation of a strengthened monopoly of power could harm those who have differing or opposing views to the central government. This is underlined by today's EU behavior. </s> The President of the European Commission Jean-Claude Juncker told European citizens that Britain will be treated as “deserters following a vote to leave the European Union. This is intimidation.  \n",
       "38495                                                                                                                                                  Difficulties with the formation of a religious identity may cause children of interfaith marriages to be pushed away from religion in general. </s> A 2006 survey showed that 37% of those raised by parents of different religions reported weekly attendance at religious services, compared with 42% of those raised by parents with the same faith.  \n",
       "91872                                                                                                                                                                                                                                                                                                           Gambling causes problems for the individual. </s> The casino owners are also individuals whom are participating in gambling during their daily operations. These people benefit from gambling.  \n",
       "92051  Religion is good for the psycho-social wellness of its followers. </s> Religion usually requires prayer, and this is the part that has shown psychological usefulness. In reducing prayer to a psychological technique of objective visualisation, you remove the need for god. This kind of prayer can work without the use of god, for example the placebo effect, or sports visualisation techniques. So these ideas show that we don't need to pray to God in order for the the techniques to work.  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10).head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b66bb76-0ea3-42da-9a22-a82fbb596f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df.sample(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0dc41-0f7e-4618-b62d-b1893f6fa21d",
   "metadata": {},
   "source": [
    "## Apply Roberta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb54124f-629c-4517-9a9d-6f21da2f153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = metric.compute(predictions=preds, references=labels)\n",
    "    return {\n",
    "        'accuracy': acc['accuracy'],\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f41b4e-dcf1-4421-b297-57d3eb8df9ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-large').cuda()\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base',num_labels=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ef3b39-8bfe-405d-b452-3d6e3de3111f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25395218bb8041c1833954e7d0aba3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d86a0e35f414f73ad18b596f14b8f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(lambda a: tokenizer(a['input_txt'], padding='max_length', max_length=256, truncation=True),batched=True)\n",
    "tokenized_valid = valid_dataset.map(lambda a: tokenizer(a['input_txt'], padding='max_length', max_length=256, truncation=True),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "156f5c27-a4f1-4a83-a5f3-52227121174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    # output_dir: directory where the model checkpoints will be saved.\n",
    "    output_dir='../data/output/stance_classification',\n",
    "    # evaluation_strategy (default \"no\"):\n",
    "    # Possible values are:\n",
    "    # \"no\": No evaluation is done during training.\n",
    "    # \"steps\": Evaluation is done (and logged) every eval_steps.\n",
    "    # \"epoch\": Evaluation is done at the end of each epoch.\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # eval_steps: Number of update steps between two evaluations if\n",
    "    # evaluation_strategy=\"steps\". Will default to the same value as\n",
    "    # logging_steps if not set.\n",
    "    eval_steps=200,\n",
    "    # logging_strategy (default: \"steps\"): The logging strategy to adopt during\n",
    "    # training (used to log training loss for example). Possible values are:\n",
    "    # \"no\": No logging is done during training.\n",
    "    # \"epoch\": Logging is done at the end of each epoch.\n",
    "    # \"steps\": Logging is done every logging_steps.\n",
    "    logging_strategy=\"steps\",\n",
    "    # logging_steps (default 500): Number of update steps between two logs if\n",
    "    # logging_strategy=\"steps\".\n",
    "    logging_steps=200,\n",
    "    # save_strategy (default \"steps\"):\n",
    "    # The checkpoint save strategy to adopt during training. Possible values are:\n",
    "    # \"no\": No save is done during training.\n",
    "    # \"epoch\": Save is done at the end of each epoch.\n",
    "    # \"steps\": Save is done every save_steps (default 500).\n",
    "    save_strategy=\"steps\",\n",
    "    # save_steps (default: 500): Number of updates steps before two checkpoint\n",
    "    # saves if save_strategy=\"steps\".\n",
    "    save_steps=600,\n",
    "    # learning_rate (default 5e-5): The initial learning rate for AdamW optimizer.\n",
    "    # Adam algorithm with weight decay fix as introduced in the paper\n",
    "    # Decoupled Weight Decay Regularization.\n",
    "    learning_rate=2e-5,\n",
    "    # per_device_train_batch_size: The batch size per GPU/TPU core/CPU for training.\n",
    "    per_device_train_batch_size=64,\n",
    "    # per_device_eval_batch_size: The batch size per GPU/TPU core/CPU for evaluation.\n",
    "    per_device_eval_batch_size=64,\n",
    "    # num_train_epochs (default 3.0): Total number of training epochs to perform\n",
    "    # (if not an integer, will perform the decimal part percents of the last epoch\n",
    "    # before stopping training).\n",
    "    num_train_epochs=3,\n",
    "    # load_best_model_at_end (default False): Whether or not to load the best model\n",
    "    # found during training at the end of training.\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model:\n",
    "    # Use in conjunction with load_best_model_at_end to specify the metric to use\n",
    "    # to compare two different models. Must be the name of a metric returned by\n",
    "    # the evaluation with or without the prefix \"eval_\".\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # report_to:\n",
    "    # The list of integrations to report the results and logs to. Supported\n",
    "    # platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"tensorboard\" and \"wandb\".\n",
    "    # Use \"all\" to report to all integrations installed, \"none\" for no integrations.\n",
    "#     report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "\n",
    "# trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_valid, compute_metrics=compute_metrics)\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    # Function that returns the model to train. It's useful to use a function\n",
    "    # instead of directly the model to make sure that we are always training\n",
    "    # an untrained model from scratch.\n",
    "    model=model,\n",
    "    # The training arguments.\n",
    "    args=args,\n",
    "    # The training dataset.\n",
    "    train_dataset=tokenized_train,\n",
    "    # The evaluation dataset. We use a small subset of the validation set\n",
    "    # composed of 150 samples to speed up computations...\n",
    "    eval_dataset=tokenized_valid.shuffle(42),#.select(range(150)),\n",
    "    # Even though the training set and evaluation set are already tokenized, the\n",
    "    # tokenizer is needed to pad the \"input_ids\" and \"attention_mask\" tensors\n",
    "    # to the length managed by the model. It does so one batch at a time, to\n",
    "    # use less memory as possible.\n",
    "    tokenizer=tokenizer,\n",
    "    # Function that will be called at the end of each evaluation phase on the whole\n",
    "    # arrays of predictions/labels to produce metrics.\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fa30f-ddd5-4b5a-8ec6-56a9c44493bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: claim2, claim1, input_txt. If claim2, claim1, input_txt are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 95664\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4485\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='4485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 215/4485 00:51 < 17:04, 4.17 it/s, Epoch 0.14/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.744419</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.779116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: claim2, claim1, input_txt, __index_level_0__. If claim2, claim1, input_txt, __index_level_0__ are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44b39a-800f-4124-8f76-9e880ab56378",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('../data/output/stance_classification/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22005fe0-408f-47a6-b16d-2dc40efdc693",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../data/output/stance_classification/best_model/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../data/output/stance_classification/best_model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../data/output/stance_classification/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../data/output/stance_classification/best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('../data/output/stance_classification/best_model').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "898ee77f-0cf8-4d34-b6b9-60636c9de7f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db34a329a1c48c182e6f81f32ce33ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: claim2, claim1, input_txt. If claim2, claim1, input_txt are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22454\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df['input_txt'] = test_df.apply(lambda x: x['claim1'] + ' </s> ' + x['claim2'], axis=1)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "tokenized_test = test_dataset.map(lambda a: tokenizer(a['input_txt'], padding='max_length', max_length=256, truncation=True),batched=True)\n",
    "pred=trainer.predict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef4520d4-e26c-4d7d-b40f-2b46265007f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.argmax(pred.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39e412b7-2d5d-4e98-8ec7-d0095d99769b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8248618784530387, 0.7978979246459428, 0.8111558835514103, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(tokenized_test['label'], scores, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126229ff-b7f1-4604-9695-4cc0a16df7be",
   "metadata": {},
   "source": [
    "### Testing on Reddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ddf4e1-49b0-42dc-b81a-1b5959059a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('../data/output/stance_classification/best_model').cuda()\n",
    "arg_stance_pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework='pt', task='ArgQ', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f252df-79a1-46f9-9476-9dce061b28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "data_fold = '../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/'\n",
    "train_df = pd.read_pickle(data_fold+'/reddit_data/conclusion_and_ca_generation/preprocessed_train_conclusion_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b21cf781-2b71-4841-ad56-63460d0152d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing sample\n",
    "sample_df = train_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fe488c2-11e7-4aac-ae9b-decbe738ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "opposing_claims = [(x, 1) for x in sample_df.apply(lambda x: x['title'] + ' </s> ' + x['counter'], axis=1).tolist()]\n",
    "supporting_claims = [(x, 0) for x in sample_df.apply(lambda x: x['title'] + ' </s> ' + ' '.join(x['post']), axis=1).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f45fd52-c000-4a2f-bb91-57b158c07235",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_claims = opposing_claims + supporting_claims\n",
    "all_claims_texts, all_claims_stances = zip(*all_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0031e6b3-eb09-42a1-a8c5-dd476e85c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_claims_pred_stances = arg_stance_pipeline(list(all_claims_texts), truncation=True)\n",
    "all_claims_pred_stances = [int(x['label'].split('_')[-1]) for x in all_claims_pred_stances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c859e2c-bf8d-4c52-a212-419f8c3a13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, _ = precision_recall_fscore_support(all_claims_stances, all_claims_pred_stances, labels=[0,1], average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca315323-f8ef-4245-8be5-56defb885fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec: 0.6292388847023361\n",
      "Rec: 0.835\n",
      "F1: 0.7176622260421144\n"
     ]
    }
   ],
   "source": [
    "print('Prec: {}'.format(p))\n",
    "print('Rec: {}'.format(r))\n",
    "print('F1: {}'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddee3b-7e02-49ca-9e2c-c2c242b36cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
